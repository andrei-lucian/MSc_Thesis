model:
  arch: transformer
  trainer: seq2seq
  width: 200
  nhead: 8
  num_layers: 6
  dropout: 0.0
  tie_decoder_embeddings: true
  max_len: 5000
dataset:
  name: iwslt14
  src_lang: de
  tgt_lang: en
  data_dir: data/iwslt14
  tokenizer_model: data/iwslt14/iwslt14_bpe.model
  src_vocab_size: 32000
  tgt_vocab_size: 32000
  pad_idx: 0
  bos_idx: 1
  eos_idx: 2
  batch_size: 64
  num_workers: 4
  max_len: 200
  subset_fraction: 0.01
training:
  epochs: 1
  batch_size: 128
  optimizer: adam
  lr: 0.0005
  momentum: 0
  weight_decay: 0.0005
  lr_schedule: inverse_sqrt
  warmup_steps: 4000
  label_smoothing: 0.1
name: transformer
seed: 0
experiment:
  name: default
  seed: 1
  output_dir: outputs/${experiment.name}
