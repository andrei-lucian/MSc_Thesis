# --- Core training control ---
use_steps: true          # use step-based training instead of epochs
max_steps: 4         # train for 80k optimizer steps
log_interval: 1       # log metrics every 1000 steps
save_interval: 1      # save every 1000 steps (or every 10 epochs for epoch-based)
save_checkpoints: true   # turn checkpoint saving on or off 

# --- Optimizer setup ---
optimizer: sgd           # stochastic gradient descent
lr: 0.005                 # standard starting LR for CIFAR10 + SGD
momentum: 0.9            # required by your baseline
weight_decay: 5e-4       # standard CIFAR regularization

# --- Scheduler ---
lr_schedule: cosine      # cosine annealing learning-rate schedule

# --- Batch and epoch bookkeeping ---
batch_size: 128          # typical CIFAR batch size
epochs: 200              # optional; only used if use_steps: false