{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_cifar100_loaders(batch_size=500):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\t])\n",
    "\t\n",
    "\ttrainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\ttestset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\t\n",
    "\ttrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\ttestloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\t\n",
    "\treturn trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\tdef __init__(self, inchannel, outchannel, stride=1):\n",
    "\t\tsuper(ResidualBlock, self).__init__()\n",
    "\n",
    "\t\tself.pre_activations1 = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel))\n",
    "\t\t\n",
    "\t\tself.pre_activations2 = nn.Sequential(\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.shortcut = nn.Sequential()\n",
    "\t\tif stride != 1 or inchannel != outchannel:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tpre_activations = self.pre_activations1(x)\n",
    "\t\tpre_activations2 = self.pre_activations2(pre_activations)\n",
    "\t\tout = pre_activations2 + self.shortcut(x)\n",
    "\t\tout = F.relu(out)\n",
    "\t\treturn out, pre_activations, pre_activations2\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\tdef __init__(self, ResidualBlock, num_classes=100):\n",
    "\t\tsuper(ResNet, self).__init__()\n",
    "\t\tself.inchannel = 64\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "\t\tself.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "\t\tself.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "\t\tself.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "\t\tself.fc = nn.Linear(512, num_classes)\n",
    "\t\t\n",
    "\tdef make_layer(self, block, channels, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1] * (num_blocks - 1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.inchannel, channels, stride))\n",
    "\t\t\tself.inchannel = channels\n",
    "\t\treturn nn.ModuleList(layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\t\n",
    "\t\tpre_activations_list = []  # Store pre-activations\n",
    "\t\t\n",
    "\t\tpre_activations_list.append(out) # Pre-activation of the first layer\n",
    "\n",
    "\t\t# Iterate through each block to capture pre-activations\n",
    "\t\tfor block in self.layer1:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer2:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\t\tfor block in self.layer3:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer4:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out, pre_activations_list\n",
    "\t\n",
    "def ResNet18():\n",
    "\treturn ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\tIdentity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "\t'''Pre-activation version of the BasicBlock.'''\n",
    "\texpansion = 1\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBlock, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "\t'''Pre-activation version of the original Bottleneck module.'''\n",
    "\texpansion = 4\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBottleneck, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout = self.conv3(F.relu(self.bn3(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "\tdef __init__(self, block, num_blocks, num_classes=10):\n",
    "\t\tsuper(PreActResNet, self).__init__()\n",
    "\t\tself.in_planes = 64\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "\t\tself.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "\t\tself.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "\t\tself.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\t\tself.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "\tdef _make_layer(self, block, planes, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1]*(num_blocks-1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.in_planes, planes, stride))\n",
    "\t\t\tself.in_planes = planes * block.expansion\n",
    "\t\treturn nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.layer1(out)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.linear(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "\treturn PreActResNet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "\t\"\"\"\n",
    "\tComputes the accuracy given predicted labels and true labels.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds (torch.Tensor): Predicted labels (tensor of shape [batch_size])\n",
    "\t\tlabels (torch.Tensor): True labels (tensor of shape [batch_size])\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tfloat: Accuracy percentage\n",
    "\t\"\"\"\n",
    "\tcorrect = (preds == labels).sum().item()  # Count correct predictions\n",
    "\ttotal = labels.size(0)  # Total number of samples\n",
    "\taccuracy = correct / total * 100  # Compute percentage\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to update the running average\n",
    "def update_running_avg(new_tensor, running_sum, count):\n",
    "\tcount += 1\n",
    "\trunning_sum += new_tensor\n",
    "\trunning_avg = running_sum / count\n",
    "\treturn running_avg, running_sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "\t\"\"\"Evaluates the model on the test dataset and computes loss & accuracy.\"\"\"\n",
    "\tmodel.eval()  # Set model to evaluation mode\n",
    "\ttotal_loss = 0.0\n",
    "\toutputs = torch.tensor([]).to(device)\n",
    "\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\n",
    "\t# Initialize running sum and count\n",
    "\trunning_sum = torch.zeros(17).to(device)  # 17 = Number of layers\n",
    "\tcount = 0\n",
    "\n",
    "\twith torch.no_grad():  # Disable gradient calculation\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\t\tfor input, label in zip(inputs, labels):\n",
    "\t\t\t\tinput, label = input.to(device), label.to(device)\n",
    "\t\t\t\toutput, pre_act = model(input.unsqueeze(0))\n",
    "\t\t\t\toutputs = torch.cat((outputs, output))\n",
    "\t\t\t\tfor layer in pre_act:\n",
    "\t\t\t\t\tmean_layer_act = torch.cat((mean_layer_act,torch.mean(layer).unsqueeze(dim=0)))\n",
    "\n",
    "\t\t\t\trunning_avg, running_sum, count = update_running_avg(mean_layer_act, running_sum, count)\n",
    "\t\t\t\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\n",
    "\t\t\tloss = criterion(outputs, labels)  # Compute loss\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\t\tpreds = torch.argmax(outputs, dim=1)  # Get predicted labels\n",
    "\t\t\t\n",
    "\t\t\tbreak # We only need one batch\n",
    "\n",
    "\tprint(running_avg)\n",
    "\tacc = compute_accuracy(preds, labels)\n",
    "\tavg_loss = total_loss / len(test_loader)  # Average loss\n",
    "\tprint(acc, avg_loss)\n",
    "\treturn avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor i, (inputs, labels) in enumerate(train_loader):\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs, _ = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tif i % 100 == 99:  # print every 100 mini-batches\n",
    "\t\t\t\ttest_dataset = test_loader.dataset  # Get the dataset from the DataLoader\n",
    "\t\t\t\tsubset_indices = random.sample(range(len(test_dataset)), 500)\n",
    "\n",
    "\t\t\t\t# Create a Subset dataset\n",
    "\t\t\t\tsubset_dataset = Subset(test_dataset, subset_indices)\n",
    "\n",
    "\t\t\t\t# Create a new DataLoader for the subset\n",
    "\t\t\t\tsubset_loader = DataLoader(subset_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "\n",
    "\t\t\t\tavg_loss, acc = test(model, device, subset_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "train_loader, test_loader = get_cifar100_loaders(124)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, 'cuda', train_loader, test_loader, criterion, optimizer, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
