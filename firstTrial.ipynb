{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.nn.functional as Fchat\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_loaders(batch_size=500):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.RandomCrop(32, padding=4),\n",
    "\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\ttransforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "\t])\n",
    "\ttrainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\ttestset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\t\n",
    "\n",
    "\t# Trim dataset to make it divisible by 5\n",
    "\t# Define split sizes (cumulative percentages: 20%, 40%, 60%, 80%, 100%)\n",
    "\ttotal_size = len(trainset)\n",
    "\tsplit_sizes = [int(total_size * p) for p in [0.2, 0.4, 0.6, 0.8, 1.0]]\n",
    "\n",
    "\t# Ensure the sum does not exceed dataset size (due to rounding)\n",
    "\tsplit_sizes[-1] = total_size  # Ensure last split gets exactly the full dataset\n",
    "\n",
    "\t# Perform the splits\n",
    "\tsub_datasets = [random_split(trainset, [s, total_size - s])[0] for s in split_sizes]\n",
    "\tsub_dataloaders = [DataLoader(subset, batch_size=batch_size, shuffle=True) for subset in sub_datasets]\n",
    "\n",
    "\ttestloader = DataLoader(testset, batch_size=500, shuffle=True, num_workers=1) #testloader always has size 500\n",
    "\t\n",
    "\treturn sub_dataloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\tdef __init__(self, inchannel, outchannel, stride=1):\n",
    "\t\tsuper(ResidualBlock, self).__init__()\n",
    "\n",
    "\t\tself.pre_activations1 = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel))\n",
    "\t\t\n",
    "\t\tself.pre_activations2 = nn.Sequential(\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.shortcut = nn.Sequential()\n",
    "\t\tif stride != 1 or inchannel != outchannel:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tpre_activations = self.pre_activations1(x)\n",
    "\t\tpre_activations2 = self.pre_activations2(pre_activations)\n",
    "\t\tout = pre_activations2 + self.shortcut(x)\n",
    "\t\tout = F.relu(out)\n",
    "\t\treturn out, pre_activations, pre_activations2\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\tdef __init__(self, ResidualBlock, num_classes=100):\n",
    "\t\tsuper(ResNet, self).__init__()\n",
    "\t\tself.inchannel = 64\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "\t\tself.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "\t\tself.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "\t\tself.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "\t\tself.fc = nn.Linear(512, num_classes)\n",
    "\t\t\n",
    "\tdef make_layer(self, block, channels, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1] * (num_blocks - 1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.inchannel, channels, stride))\n",
    "\t\t\tself.inchannel = channels\n",
    "\t\treturn nn.ModuleList(layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\t\n",
    "\t\tpre_activations_list = []  # Store pre-activations\n",
    "\t\t\n",
    "\t\tpre_activations_list.append(out) # Pre-activation of the first layer\n",
    "\n",
    "\t\t# Iterate through each block to capture pre-activations\n",
    "\t\tfor block in self.layer1:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer2:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\t\tfor block in self.layer3:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer4:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out, pre_activations_list\n",
    "\t\n",
    "def ResNet18():\n",
    "\treturn ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\tIdentity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "\t'''Pre-activation version of the BasicBlock.'''\n",
    "\texpansion = 1\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBlock, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "\t'''Pre-activation version of the original Bottleneck module.'''\n",
    "\texpansion = 4\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBottleneck, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout = self.conv3(F.relu(self.bn3(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "\tdef __init__(self, block, num_blocks, num_classes=10):\n",
    "\t\tsuper(PreActResNet, self).__init__()\n",
    "\t\tself.in_planes = 64\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "\t\tself.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "\t\tself.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "\t\tself.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\t\tself.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "\tdef _make_layer(self, block, planes, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1]*(num_blocks-1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.in_planes, planes, stride))\n",
    "\t\t\tself.in_planes = planes * block.expansion\n",
    "\t\treturn nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.layer1(out)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.linear(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "\treturn PreActResNet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "\t\"\"\"\n",
    "\tComputes the accuracy given predicted labels and true labels.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds (torch.Tensor): Predicted labels (tensor of shape [batch_size])\n",
    "\t\tlabels (torch.Tensor): True labels (tensor of shape [batch_size])\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tfloat: Accuracy percentage\n",
    "\t\"\"\"\n",
    "\tcorrect = (preds == labels).sum().item()  # Count correct predictions\n",
    "\ttotal = labels.size(0)  # Total number of samples\n",
    "\taccuracy = correct / total * 100  # Compute percentage\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the running average\n",
    "def update_running_avg(new_tensor, running_sum, count):\n",
    "\tcount += 1\n",
    "\trunning_sum += new_tensor\n",
    "\trunning_avg = running_sum / count\n",
    "\treturn running_avg, running_sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, data_file, epoch, run, data_counter):\n",
    "\t\"\"\"Evaluates the model on the test dataset and computes loss & accuracy.\"\"\"\n",
    "\tmodel.eval()  # Set model to evaluation mode\n",
    "\ttotal_loss = 0.0\n",
    "\toutputs = torch.tensor([]).to(device)\n",
    "\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\t\n",
    "\tcolumn_names = [f'Layer{i+1}' for i in range(17)] # Define column names for csv file\n",
    "\tcolumn_names.extend(['acc', 'avg_loss', 'epoch', 'run', 'ammount_of_data']) # Add extra info columns\n",
    "\n",
    "\t# Initialize running sum and count\n",
    "\trunning_sum = torch.zeros(17).to(device)  # 17 = Number of layers\n",
    "\tcount = 0\n",
    "\n",
    "\twith torch.no_grad():  # Disable gradient calculation\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\t\tfor input, label in zip(inputs, labels):\n",
    "\t\t\t\tinput, label = input.to(device), label.to(device)\n",
    "\t\t\t\toutput, pre_act = model(input.unsqueeze(0))\n",
    "\t\t\t\toutputs = torch.cat((outputs, output))\n",
    "\t\t\t\tfor layer in pre_act:\n",
    "\t\t\t\t\tmean_layer_act = torch.cat((mean_layer_act,torch.mean(layer).unsqueeze(dim=0)))\n",
    "\n",
    "\t\t\t\trunning_avg, running_sum, count = update_running_avg(mean_layer_act, running_sum, count)\n",
    "\t\t\t\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\n",
    "\t\t\tloss = criterion(outputs, labels)  # Compute loss\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\t\tpreds = torch.argmax(outputs, dim=1)  # Get predicted labels\n",
    "\t\t\t\n",
    "\t\t\tbreak # We only need one batch\n",
    "\n",
    "\tacc = compute_accuracy(preds, labels)\n",
    "\tavg_loss = total_loss / len(test_loader)  # Average loss\n",
    "\n",
    "\trunning_avg = torch.cat((running_avg, torch.tensor([acc, avg_loss, epoch, run, data_counter]).to(device))) # Add current epoch and run to the features\n",
    "\t\n",
    "    # Convert to DataFrame (single row)\n",
    "\tdf = pd.DataFrame([running_avg.cpu().numpy()], columns=column_names)\n",
    "\n",
    "    # Append to CSV, write header only if the file does not exist\n",
    "\tdf.to_csv(data_file, mode='a', header=not os.path.exists(data_file), index=False)\n",
    "\n",
    "\tprint(acc, avg_loss)\n",
    "\treturn avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait before stopping if no improvement.\n",
    "            min_delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')  # Track best loss\n",
    "        self.counter = 0  # Count epochs without improvement\n",
    "\n",
    "    def __call__(self, avg_loss):\n",
    "        \"\"\"Returns True if training should stop.\"\"\"\n",
    "        if avg_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = avg_loss  # Update best loss\n",
    "            self.counter = 0  # Reset counter\n",
    "        else:\n",
    "            self.counter += 1  # Increase counter if no improvement\n",
    "        \n",
    "        return self.counter >= self.patience  # Stop if patience is exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, criterion, optimizer, scheduler, run, data_counter, epochs=5):\n",
    "\tmodel.train()\n",
    "\t# Define CSV file path\n",
    "\tdata_file = 'activations_per_layer.csv'\n",
    "\tearly_stopping = EarlyStopping(patience=5, min_delta=0.001)  # Adjust patience & delta\n",
    "\n",
    "\tdataset_size = len(train_loader.dataset)  # Get total dataset size\n",
    "\tprogress_interval = int(dataset_size * 0.05)  # Compute 5% of dataset\n",
    "\tcount = 0  # Initialize sample counter\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tfor i, (inputs, labels) in enumerate(train_loader):\n",
    "\t\t\tcount += len(inputs)  # Increment count by batch size\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs, _ = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\tif count % progress_interval < len(inputs):  # Print every 5% of dataset\n",
    "\t\t\tavg_loss, acc = test(model, device, test_loader, criterion, data_file, epoch, run, data_counter)\n",
    "\t\t\n",
    "\t\t# scheduler.step()\n",
    "\t\t\n",
    "\t\tif early_stopping(avg_loss):\n",
    "\t\t\tprint(\"Early stopping triggered. Stopping training.\")\n",
    "\t\t\tbreak  # Stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "8.0 0.20229177474975585\n",
      "14.6 0.1854434370994568\n",
      "18.2 0.1718930721282959\n",
      "21.8 0.16154892444610597\n",
      "24.8 0.14801515340805055\n",
      "30.2 0.14323464632034302\n",
      "36.4 0.12892850637435913\n",
      "35.4 0.12934781312942506\n",
      "37.8 0.11497180461883545\n",
      "37.4 0.11376030445098877\n",
      "35.8 0.12113080024719239\n",
      "43.4 0.10109421014785766\n",
      "42.0 0.1151163101196289\n",
      "42.8 0.10508381128311158\n",
      "48.0 0.10023542642593383\n",
      "51.800000000000004 0.09421571493148803\n",
      "48.6 0.09755761623382568\n",
      "50.8 0.09574383497238159\n",
      "43.8 0.10637671947479248\n",
      "50.8 0.09264458417892456\n",
      "50.4 0.10064467191696166\n",
      "54.2 0.09232300519943237\n",
      "50.6 0.09110703468322753\n",
      "53.800000000000004 0.08476624488830567\n",
      "51.0 0.1003179669380188\n",
      "55.60000000000001 0.09253910779953003\n",
      "54.400000000000006 0.08946606516838074\n",
      "53.800000000000004 0.08801878690719604\n",
      "54.800000000000004 0.0934946894645691\n",
      "Early stopping triggered. Stopping training.\n",
      "8.0 0.20404341220855712\n",
      "9.2 0.19532035589218139\n",
      "15.0 0.17980785369873048\n",
      "19.400000000000002 0.17341984510421754\n",
      "22.8 0.15783125162124634\n",
      "25.8 0.14817969799041747\n",
      "33.0 0.13019486665725707\n",
      "32.800000000000004 0.12911324501037597\n",
      "38.0 0.12129789590835571\n",
      "43.0 0.11194993257522583\n",
      "43.4 0.11026986837387084\n",
      "43.0 0.10857688188552857\n",
      "43.0 0.10545532703399658\n",
      "41.199999999999996 0.10664374828338623\n",
      "45.2 0.10427823066711425\n",
      "50.6 0.10111714601516723\n",
      "49.8 0.0974826157093048\n",
      "43.6 0.10385123491287232\n",
      "51.4 0.09336934089660645\n",
      "47.599999999999994 0.10256987810134888\n",
      "53.800000000000004 0.09646742939949035\n",
      "48.0 0.09898940324783326\n",
      "50.0 0.09208064675331115\n",
      "50.6 0.09416156411170959\n",
      "51.0 0.09592039585113525\n",
      "54.2 0.09614993929862976\n",
      "54.400000000000006 0.08952153921127319\n",
      "50.4 0.09606929421424866\n",
      "54.6 0.09002221822738647\n",
      "56.599999999999994 0.09401761889457702\n",
      "49.6 0.10136179924011231\n",
      "49.6 0.10581003427505493\n",
      "Early stopping triggered. Stopping training.\n",
      "9.0 0.20426132678985595\n",
      "11.200000000000001 0.1921040892601013\n",
      "19.6 0.17476860284805298\n",
      "24.2 0.16245628595352174\n",
      "29.4 0.1454770565032959\n",
      "29.2 0.14028556346893312\n",
      "32.6 0.12818553447723388\n",
      "35.0 0.1252753734588623\n",
      "41.0 0.11195405721664428\n",
      "43.8 0.10923489332199096\n",
      "49.2 0.1006253719329834\n",
      "42.8 0.11472400426864623\n",
      "45.0 0.10831184387207031\n",
      "47.4 0.10225419998168946\n",
      "45.4 0.10450919866561889\n",
      "49.6 0.09407442212104797\n",
      "48.6 0.09848732948303222\n",
      "50.0 0.09244652986526489\n",
      "47.199999999999996 0.10834527015686035\n",
      "50.4 0.09306736588478089\n",
      "52.800000000000004 0.09259160757064819\n",
      "52.6 0.09220411777496337\n",
      "51.0 0.092291259765625\n",
      "Early stopping triggered. Stopping training.\n",
      "6.2 0.20946755409240722\n",
      "8.799999999999999 0.19578144550323487\n",
      "15.2 0.18072339296340942\n",
      "21.6 0.16578028202056885\n",
      "26.8 0.15160681009292604\n",
      "29.4 0.14029499292373657\n",
      "31.0 0.1399290680885315\n",
      "35.6 0.12826181650161744\n",
      "38.0 0.12247270345687866\n",
      "40.2 0.11232383251190185\n",
      "43.2 0.10934464931488037\n",
      "44.2 0.10651429891586303\n",
      "42.8 0.10951076745986939\n",
      "43.0 0.10664390325546265\n",
      "49.0 0.10359373092651367\n",
      "44.2 0.11015937328338624\n",
      "45.4 0.10693482160568238\n",
      "50.8 0.10036563873291016\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "# model = ResNet18()\n",
    "# model.to(device)\n",
    "num_runs = 5\n",
    "\n",
    "train_loaders, test_loader = get_cifar100_loaders(128)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_counter = 0\n",
    "for train_loader in train_loaders:\n",
    "\tif data_counter >= 3:\n",
    "\t\tfor run in range(num_runs):\n",
    "\t\t\tif run == 0 and data_counter == 3:\n",
    "\t\t\t\trun += 1\n",
    "\t\t\tmodel = ResNet18()\n",
    "\t\t\tmodel.to(device)\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\t\t\t# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)  # For 200 epochs\n",
    "\n",
    "\t\t\ttrain(model, device, train_loader, test_loader, criterion, optimizer, None, run, data_counter, epochs=100) #Train for x epochs or until early stopping\n",
    "\n",
    "\tdata_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5f2dcc6628d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace 'your_file.csv' with the path to your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolumns_to_exclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace with actual column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'activations_per_layer.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "columns_to_exclude = ['acc', 'avg_loss', 'epoch', 'run']  # Replace with actual column names\n",
    "df = pd.read_csv('activations_per_layer.csv')\n",
    "\n",
    "\n",
    "# Drop the specified columns and compute the row-wise mean for the remaining columns\n",
    "df['average'] = df.drop(columns=columns_to_exclude).mean(axis=1)\n",
    "\n",
    "\n",
    "# df_avg['average'] = df_avg.mean(axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'time' column based on the order (index) of the rows\n",
    "df['time'] = df.index // 2  # Assuming 3 runs per time point\n",
    "\n",
    "# Calculate the mean and standard deviation for each time point\n",
    "time_means = df.groupby('time')['average'].mean()\n",
    "time_stds = df.groupby('time')['average'].std()\n",
    "\n",
    "# Convert the mean and std into DataFrame for plotting\n",
    "time_means_df = pd.DataFrame({'time': time_means.index, 'mean': time_means.values})\n",
    "time_stds_df = pd.DataFrame({'time': time_stds.index, 'std': time_stds.values})\n",
    "\n",
    "print(time_means_df)\n",
    "print(time_stds_df)\n",
    "\n",
    "# print(time_means_df.iloc[155:165])\n",
    "\n",
    "# Plot the time series with error bands\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the mean values\n",
    "sns.lineplot(x='time', y='mean', data=time_means_df, label='Mean', color='b')\n",
    "\n",
    "# Add error bands\n",
    "plt.fill_between(time_stds_df['time'], \n",
    "                 time_means_df['mean'] - time_stds_df['std'], \n",
    "                 time_means_df['mean'] + time_stds_df['std'], \n",
    "                 color='blue', alpha=0.3)\n",
    "\n",
    "plt.title(\"Time Series with Error Bands (Standard Deviation)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
