{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as Fchat\n",
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_loaders(batch_size=500):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\t])\n",
    "\t\n",
    "\ttrainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\ttestset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\t\n",
    "\ttrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\ttestloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\t\n",
    "\treturn trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\tdef __init__(self, inchannel, outchannel, stride=1):\n",
    "\t\tsuper(ResidualBlock, self).__init__()\n",
    "\n",
    "\t\tself.pre_activations1 = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel))\n",
    "\t\t\n",
    "\t\tself.pre_activations2 = nn.Sequential(\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.shortcut = nn.Sequential()\n",
    "\t\tif stride != 1 or inchannel != outchannel:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tpre_activations = self.pre_activations1(x)\n",
    "\t\tpre_activations2 = self.pre_activations2(pre_activations)\n",
    "\t\tout = pre_activations2 + self.shortcut(x)\n",
    "\t\tout = F.relu(out)\n",
    "\t\treturn out, pre_activations, pre_activations2\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\tdef __init__(self, ResidualBlock, num_classes=100):\n",
    "\t\tsuper(ResNet, self).__init__()\n",
    "\t\tself.inchannel = 64\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "\t\tself.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "\t\tself.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "\t\tself.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "\t\tself.fc = nn.Linear(512, num_classes)\n",
    "\t\t\n",
    "\tdef make_layer(self, block, channels, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1] * (num_blocks - 1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.inchannel, channels, stride))\n",
    "\t\t\tself.inchannel = channels\n",
    "\t\treturn nn.ModuleList(layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\t\n",
    "\t\tpre_activations_list = []  # Store pre-activations\n",
    "\t\t\n",
    "\t\tpre_activations_list.append(out) # Pre-activation of the first layer\n",
    "\n",
    "\t\t# Iterate through each block to capture pre-activations\n",
    "\t\tfor block in self.layer1:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer2:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\t\tfor block in self.layer3:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer4:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out, pre_activations_list\n",
    "\t\n",
    "def ResNet18():\n",
    "\treturn ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\tIdentity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "\t'''Pre-activation version of the BasicBlock.'''\n",
    "\texpansion = 1\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBlock, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "\t'''Pre-activation version of the original Bottleneck module.'''\n",
    "\texpansion = 4\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBottleneck, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout = self.conv3(F.relu(self.bn3(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "\tdef __init__(self, block, num_blocks, num_classes=10):\n",
    "\t\tsuper(PreActResNet, self).__init__()\n",
    "\t\tself.in_planes = 64\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "\t\tself.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "\t\tself.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "\t\tself.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\t\tself.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "\tdef _make_layer(self, block, planes, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1]*(num_blocks-1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.in_planes, planes, stride))\n",
    "\t\t\tself.in_planes = planes * block.expansion\n",
    "\t\treturn nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.layer1(out)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.linear(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "\treturn PreActResNet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "\t\"\"\"\n",
    "\tComputes the accuracy given predicted labels and true labels.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds (torch.Tensor): Predicted labels (tensor of shape [batch_size])\n",
    "\t\tlabels (torch.Tensor): True labels (tensor of shape [batch_size])\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tfloat: Accuracy percentage\n",
    "\t\"\"\"\n",
    "\tcorrect = (preds == labels).sum().item()  # Count correct predictions\n",
    "\ttotal = labels.size(0)  # Total number of samples\n",
    "\taccuracy = correct / total * 100  # Compute percentage\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the running average\n",
    "def update_running_avg(new_tensor, running_sum, count):\n",
    "\tcount += 1\n",
    "\trunning_sum += new_tensor\n",
    "\trunning_avg = running_sum / count\n",
    "\treturn running_avg, running_sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, data_file, epoch, run):\n",
    "\t\"\"\"Evaluates the model on the test dataset and computes loss & accuracy.\"\"\"\n",
    "\tmodel.eval()  # Set model to evaluation mode\n",
    "\ttotal_loss = 0.0\n",
    "\toutputs = torch.tensor([]).to(device)\n",
    "\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\t\n",
    "\tcolumn_names = [f'Layer{i+1}' for i in range(17)] # Define column names for csv file\n",
    "\tcolumn_names.extend(['acc', 'avg_loss', 'epoch', 'run']) # Add extra info columns\n",
    "\n",
    "\t# Initialize running sum and count\n",
    "\trunning_sum = torch.zeros(17).to(device)  # 17 = Number of layers\n",
    "\tcount = 0\n",
    "\n",
    "\twith torch.no_grad():  # Disable gradient calculation\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\t\tfor input, label in zip(inputs, labels):\n",
    "\t\t\t\tinput, label = input.to(device), label.to(device)\n",
    "\t\t\t\toutput, pre_act = model(input.unsqueeze(0))\n",
    "\t\t\t\toutputs = torch.cat((outputs, output))\n",
    "\t\t\t\tfor layer in pre_act:\n",
    "\t\t\t\t\tmean_layer_act = torch.cat((mean_layer_act,torch.mean(layer).unsqueeze(dim=0)))\n",
    "\n",
    "\t\t\t\trunning_avg, running_sum, count = update_running_avg(mean_layer_act, running_sum, count)\n",
    "\t\t\t\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\n",
    "\t\t\tloss = criterion(outputs, labels)  # Compute loss\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\t\tpreds = torch.argmax(outputs, dim=1)  # Get predicted labels\n",
    "\t\t\t\n",
    "\t\t\tbreak # We only need one batch\n",
    "\n",
    "\tacc = compute_accuracy(preds, labels)\n",
    "\tavg_loss = total_loss / len(test_loader)  # Average loss\n",
    "\n",
    "\trunning_avg = torch.cat((running_avg, torch.tensor([acc, avg_loss, epoch, run]).to(device))) # Add current epoch and run to the features\n",
    "\t\n",
    "    # Convert to DataFrame (single row)\n",
    "\tdf = pd.DataFrame([running_avg.cpu().numpy()], columns=column_names)\n",
    "\n",
    "    # Append to CSV, write header only if the file does not exist\n",
    "\tdf.to_csv(data_file, mode='a', header=not os.path.exists(data_file), index=False)\n",
    "\n",
    "\tprint(acc, avg_loss)\n",
    "\treturn avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, criterion, optimizer, run, epochs=5):\n",
    "\tmodel.train()\n",
    "\n",
    "\t# Define CSV file path\n",
    "\tdata_file = 'activations_per_layer.csv'\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor i, (inputs, labels) in enumerate(train_loader):\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs, _ = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tif i % 5 == 4:  # Print every 5 mini-batches\n",
    "\t\t\t\ttest_dataset = test_loader.dataset  # Get the dataset from the DataLoader\n",
    "\t\t\t\tsubset_indices = random.sample(range(len(test_dataset)), 500)\n",
    "\n",
    "\t\t\t\t# Create a Subset dataset\n",
    "\t\t\t\tsubset_dataset = Subset(test_dataset, subset_indices)\n",
    "\n",
    "\t\t\t\t# Create a new DataLoader for the subset\n",
    "\t\t\t\tsubset_loader = DataLoader(subset_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "\n",
    "\t\t\t\tavg_loss, acc = test(model, device, subset_loader, criterion, data_file, epoch, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "1.5625 0.5759532451629639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \t\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, test_loader, criterion, optimizer, run, epochs)\u001b[39m\n\u001b[32m     12\u001b[39m outputs, _ = model(inputs)\n\u001b[32m     13\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m optimizer.step()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m5\u001b[39m == \u001b[32m4\u001b[39m:  \u001b[38;5;66;03m# Print every 5 mini-batches\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    479\u001b[39m         Tensor.backward,\n\u001b[32m    480\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m         inputs=inputs,\n\u001b[32m    486\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    195\u001b[39m     retain_graph = create_graph\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = ResNet18()\n",
    "model.to(device)\n",
    "num_runs = 2\n",
    "\n",
    "train_loader, test_loader = get_cifar100_loaders(64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for run in range(num_runs):\n",
    "\ttrain(model, device, train_loader, test_loader, criterion, optimizer, run, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
