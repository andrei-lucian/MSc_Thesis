{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as Fchat\n",
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_loaders(batch_size=500):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "\t])\n",
    "\t\n",
    "\ttrainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\ttestset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\t\n",
    "\ttrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\ttestloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\t\n",
    "\treturn trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\tdef __init__(self, inchannel, outchannel, stride=1):\n",
    "\t\tsuper(ResidualBlock, self).__init__()\n",
    "\n",
    "\t\tself.pre_activations1 = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel))\n",
    "\t\t\n",
    "\t\tself.pre_activations2 = nn.Sequential(\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.shortcut = nn.Sequential()\n",
    "\t\tif stride != 1 or inchannel != outchannel:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "\t\t\t\tnn.BatchNorm2d(outchannel)\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tpre_activations = self.pre_activations1(x)\n",
    "\t\tpre_activations2 = self.pre_activations2(pre_activations)\n",
    "\t\tout = pre_activations2 + self.shortcut(x)\n",
    "\t\tout = F.relu(out)\n",
    "\t\treturn out, pre_activations, pre_activations2\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\tdef __init__(self, ResidualBlock, num_classes=100):\n",
    "\t\tsuper(ResNet, self).__init__()\n",
    "\t\tself.inchannel = 64\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "\t\tself.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "\t\tself.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "\t\tself.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "\t\tself.fc = nn.Linear(512, num_classes)\n",
    "\t\t\n",
    "\tdef make_layer(self, block, channels, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1] * (num_blocks - 1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.inchannel, channels, stride))\n",
    "\t\t\tself.inchannel = channels\n",
    "\t\treturn nn.ModuleList(layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\t\n",
    "\t\tpre_activations_list = []  # Store pre-activations\n",
    "\t\t\n",
    "\t\tpre_activations_list.append(out) # Pre-activation of the first layer\n",
    "\n",
    "\t\t# Iterate through each block to capture pre-activations\n",
    "\t\tfor block in self.layer1:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer2:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\t\tfor block in self.layer3:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\n",
    "\t\tfor block in self.layer4:\n",
    "\t\t\tout, pre_act, pre_act2 = block(out)\n",
    "\t\t\tpre_activations_list.append(pre_act)\n",
    "\t\t\tpre_activations_list.append(pre_act2)\t\n",
    "\t\t\n",
    "\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out, pre_activations_list\n",
    "\t\n",
    "def ResNet18():\n",
    "\treturn ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "\tIdentity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "\t'''Pre-activation version of the BasicBlock.'''\n",
    "\texpansion = 1\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBlock, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "\t'''Pre-activation version of the original Bottleneck module.'''\n",
    "\texpansion = 4\n",
    "\n",
    "\tdef __init__(self, in_planes, planes, stride=1):\n",
    "\t\tsuper(PreActBottleneck, self).__init__()\n",
    "\t\tself.bn1 = nn.BatchNorm2d(in_planes)\n",
    "\t\tself.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(planes)\n",
    "\t\tself.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "\t\tif stride != 1 or in_planes != self.expansion*planes:\n",
    "\t\t\tself.shortcut = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.bn1(x))\n",
    "\t\tshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "\t\tout = self.conv1(out)\n",
    "\t\tout = self.conv2(F.relu(self.bn2(out)))\n",
    "\t\tout = self.conv3(F.relu(self.bn3(out)))\n",
    "\t\tout += shortcut\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "\tdef __init__(self, block, num_blocks, num_classes=10):\n",
    "\t\tsuper(PreActResNet, self).__init__()\n",
    "\t\tself.in_planes = 64\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\t\tself.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "\t\tself.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "\t\tself.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "\t\tself.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\t\tself.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "\tdef _make_layer(self, block, planes, num_blocks, stride):\n",
    "\t\tstrides = [stride] + [1]*(num_blocks-1)\n",
    "\t\tlayers = []\n",
    "\t\tfor stride in strides:\n",
    "\t\t\tlayers.append(block(self.in_planes, planes, stride))\n",
    "\t\t\tself.in_planes = planes * block.expansion\n",
    "\t\treturn nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.conv1(x)\n",
    "\t\tout = self.layer1(out)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = F.avg_pool2d(out, 4)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.linear(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "\treturn PreActResNet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "\t\"\"\"\n",
    "\tComputes the accuracy given predicted labels and true labels.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tpreds (torch.Tensor): Predicted labels (tensor of shape [batch_size])\n",
    "\t\tlabels (torch.Tensor): True labels (tensor of shape [batch_size])\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tfloat: Accuracy percentage\n",
    "\t\"\"\"\n",
    "\tcorrect = (preds == labels).sum().item()  # Count correct predictions\n",
    "\ttotal = labels.size(0)  # Total number of samples\n",
    "\taccuracy = correct / total * 100  # Compute percentage\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the running average\n",
    "def update_running_avg(new_tensor, running_sum, count):\n",
    "\tcount += 1\n",
    "\trunning_sum += new_tensor\n",
    "\trunning_avg = running_sum / count\n",
    "\treturn running_avg, running_sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, data_file, epoch, run):\n",
    "\t\"\"\"Evaluates the model on the test dataset and computes loss & accuracy.\"\"\"\n",
    "\tmodel.eval()  # Set model to evaluation mode\n",
    "\ttotal_loss = 0.0\n",
    "\toutputs = torch.tensor([]).to(device)\n",
    "\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\t\n",
    "\tcolumn_names = [f'Layer{i+1}' for i in range(17)] # Define column names for csv file\n",
    "\tcolumn_names.extend(['acc', 'avg_loss', 'epoch', 'run']) # Add extra info columns\n",
    "\n",
    "\t# Initialize running sum and count\n",
    "\trunning_sum = torch.zeros(17).to(device)  # 17 = Number of layers\n",
    "\tcount = 0\n",
    "\n",
    "\twith torch.no_grad():  # Disable gradient calculation\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\t\tfor input, label in zip(inputs, labels):\n",
    "\t\t\t\tinput, label = input.to(device), label.to(device)\n",
    "\t\t\t\toutput, pre_act = model(input.unsqueeze(0))\n",
    "\t\t\t\toutputs = torch.cat((outputs, output))\n",
    "\t\t\t\tfor layer in pre_act:\n",
    "\t\t\t\t\tmean_layer_act = torch.cat((mean_layer_act,torch.mean(layer).unsqueeze(dim=0)))\n",
    "\n",
    "\t\t\t\trunning_avg, running_sum, count = update_running_avg(mean_layer_act, running_sum, count)\n",
    "\t\t\t\tmean_layer_act = torch.tensor([]).to(device)\n",
    "\n",
    "\t\t\tloss = criterion(outputs, labels)  # Compute loss\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\t\tpreds = torch.argmax(outputs, dim=1)  # Get predicted labels\n",
    "\t\t\t\n",
    "\t\t\tbreak # We only need one batch\n",
    "\n",
    "\tacc = compute_accuracy(preds, labels)\n",
    "\tavg_loss = total_loss / len(test_loader)  # Average loss\n",
    "\n",
    "\trunning_avg = torch.cat((running_avg, torch.tensor([acc, avg_loss, epoch, run]).to(device))) # Add current epoch and run to the features\n",
    "\t\n",
    "    # Convert to DataFrame (single row)\n",
    "\tdf = pd.DataFrame([running_avg.cpu().numpy()], columns=column_names)\n",
    "\n",
    "    # Append to CSV, write header only if the file does not exist\n",
    "\tdf.to_csv(data_file, mode='a', header=not os.path.exists(data_file), index=False)\n",
    "\n",
    "\tprint(acc, avg_loss)\n",
    "\treturn avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, criterion, optimizer, run, epochs=5):\n",
    "\tmodel.train()\n",
    "\n",
    "\t# Define CSV file path\n",
    "\tdata_file = 'activations_per_layer.csv'\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor i, (inputs, labels) in enumerate(train_loader):\n",
    "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs, _ = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tif i % 5 == 4:  # Print every 5 mini-batches\n",
    "\t\t\t\ttest_dataset = test_loader.dataset  # Get the dataset from the DataLoader\n",
    "\t\t\t\tsubset_indices = random.sample(range(len(test_dataset)), 500)\n",
    "\n",
    "\t\t\t\t# Create a Subset dataset\n",
    "\t\t\t\tsubset_dataset = Subset(test_dataset, subset_indices)\n",
    "\n",
    "\t\t\t\t# Create a new DataLoader for the subset\n",
    "\t\t\t\tsubset_loader = DataLoader(subset_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "\n",
    "\t\t\t\tavg_loss, acc = test(model, device, subset_loader, criterion, data_file, epoch, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5625 0.8415665030479431\n",
      "0.0 0.5845010876655579\n",
      "1.5625 0.5797569155693054\n",
      "0.0 0.5697618722915649\n",
      "0.0 0.5794532299041748\n",
      "0.0 0.5746384263038635\n",
      "1.5625 0.5775668025016785\n",
      "0.0 0.5769810676574707\n",
      "0.0 0.5680952072143555\n",
      "4.6875 0.5728590488433838\n",
      "1.5625 0.557929515838623\n",
      "3.125 0.5507879257202148\n",
      "1.5625 0.578641414642334\n",
      "1.5625 0.5435731410980225\n",
      "4.6875 0.5636532306671143\n",
      "1.5625 0.5665848851203918\n",
      "1.5625 0.5616368055343628\n",
      "1.5625 0.5612660050392151\n",
      "3.125 0.5548491477966309\n",
      "3.125 0.561765730381012\n",
      "1.5625 0.5627942681312561\n",
      "3.125 0.559289276599884\n",
      "4.6875 0.5526705980300903\n",
      "1.5625 0.5590170621871948\n",
      "4.6875 0.5457910895347595\n",
      "6.25 0.5359788537025452\n",
      "1.5625 0.5626955032348633\n",
      "7.8125 0.522275447845459\n",
      "6.25 0.5302754044532776\n",
      "4.6875 0.5198038220405579\n",
      "4.6875 0.562760591506958\n",
      "1.5625 0.5443183779716492\n",
      "6.25 0.5214754343032837\n",
      "1.5625 0.5126492977142334\n",
      "4.6875 0.5066198706626892\n",
      "6.25 0.5287662744522095\n",
      "0.0 0.5442607402801514\n",
      "1.5625 0.523103654384613\n",
      "0.0 0.5294123888015747\n",
      "7.8125 0.527579665184021\n",
      "3.125 0.5144469738006592\n",
      "3.125 0.5337487459182739\n",
      "4.6875 0.5199227333068848\n",
      "4.6875 0.513776421546936\n",
      "12.5 0.513386070728302\n",
      "4.6875 0.5067925453186035\n",
      "1.5625 0.5326462388038635\n",
      "4.6875 0.520298957824707\n",
      "3.125 0.5220199823379517\n",
      "9.375 0.5277490615844727\n",
      "0.0 0.5235583782196045\n",
      "9.375 0.49152231216430664\n",
      "7.8125 0.5067470073699951\n",
      "1.5625 0.5018061995506287\n",
      "1.5625 0.48996636271476746\n",
      "7.8125 0.49284055829048157\n",
      "1.5625 0.5086178183555603\n",
      "7.8125 0.5185661315917969\n",
      "7.8125 0.4893922805786133\n",
      "4.6875 0.5210887789726257\n",
      "3.125 0.5122952461242676\n",
      "3.125 0.5100571513175964\n",
      "9.375 0.4972785413265228\n",
      "4.6875 0.5065509676933289\n",
      "4.6875 0.535594642162323\n",
      "10.9375 0.4816400706768036\n",
      "4.6875 0.5088684558868408\n",
      "7.8125 0.5012772083282471\n",
      "4.6875 0.5116173624992371\n",
      "4.6875 0.5077974796295166\n",
      "6.25 0.4837997853755951\n",
      "6.25 0.49856168031692505\n",
      "7.8125 0.48041093349456787\n",
      "3.125 0.5152409076690674\n",
      "7.8125 0.48689961433410645\n",
      "6.25 0.5009424686431885\n",
      "4.6875 0.5115216970443726\n",
      "7.8125 0.49255844950675964\n",
      "10.9375 0.4874786138534546\n",
      "4.6875 0.492919385433197\n",
      "15.625 0.46064403653144836\n",
      "3.125 0.4871293008327484\n",
      "6.25 0.5154836773872375\n",
      "3.125 0.5006151795387268\n",
      "6.25 0.511350154876709\n",
      "9.375 0.49876970052719116\n",
      "9.375 0.4974753260612488\n",
      "6.25 0.4827059209346771\n",
      "10.9375 0.4804481267929077\n",
      "6.25 0.48842161893844604\n",
      "7.8125 0.4768640995025635\n",
      "4.6875 0.5271950960159302\n",
      "7.8125 0.5042779445648193\n",
      "10.9375 0.47040316462516785\n",
      "4.6875 0.5122571587562561\n",
      "9.375 0.4908043444156647\n",
      "7.8125 0.48236846923828125\n",
      "9.375 0.4786544740200043\n",
      "12.5 0.5084646344184875\n",
      "3.125 0.47242507338523865\n",
      "10.9375 0.49926409125328064\n",
      "3.125 0.5159659385681152\n",
      "7.8125 0.501848042011261\n",
      "7.8125 0.4908777177333832\n",
      "7.8125 0.4737794101238251\n",
      "14.0625 0.4781940281391144\n",
      "10.9375 0.4824337363243103\n",
      "9.375 0.4671444594860077\n",
      "10.9375 0.4829704761505127\n",
      "7.8125 0.4886011481285095\n",
      "3.125 0.4787468910217285\n",
      "14.0625 0.4837913513183594\n",
      "14.0625 0.4779190421104431\n",
      "1.5625 0.4870952367782593\n",
      "14.0625 0.48435473442077637\n",
      "9.375 0.4785309433937073\n",
      "7.8125 0.5149571299552917\n",
      "6.25 0.4773836135864258\n",
      "7.8125 0.46123403310775757\n",
      "14.0625 0.4709094762802124\n",
      "12.5 0.47507575154304504\n",
      "10.9375 0.4698057174682617\n",
      "9.375 0.4729423224925995\n",
      "4.6875 0.4870096743106842\n",
      "15.625 0.4397958219051361\n",
      "3.125 0.5047391057014465\n",
      "7.8125 0.49516981840133667\n",
      "7.8125 0.4861382246017456\n",
      "7.8125 0.4693254828453064\n",
      "14.0625 0.48575496673583984\n",
      "12.5 0.44410353899002075\n",
      "15.625 0.4284608066082001\n",
      "10.9375 0.481945276260376\n",
      "7.8125 0.4807717204093933\n",
      "10.9375 0.43934741616249084\n",
      "4.6875 0.5111452341079712\n",
      "17.1875 0.44190147519111633\n",
      "17.1875 0.47254306077957153\n",
      "6.25 0.4872773587703705\n",
      "9.375 0.4741504490375519\n",
      "15.625 0.4274151921272278\n",
      "9.375 0.4715460240840912\n",
      "10.9375 0.4708064794540405\n",
      "7.8125 0.4858643412590027\n",
      "20.3125 0.42546597123146057\n",
      "12.5 0.47329849004745483\n",
      "12.5 0.4226045608520508\n",
      "15.625 0.43859729170799255\n",
      "10.9375 0.48764345049858093\n",
      "10.9375 0.5219824314117432\n",
      "10.9375 0.43161019682884216\n",
      "14.0625 0.4547378420829773\n",
      "10.9375 0.48019567131996155\n",
      "18.75 0.4511187672615051\n",
      "3.125 0.4548816382884979\n",
      "15.625 0.44728001952171326\n",
      "9.375 0.46754342317581177\n",
      "9.375 0.48030102252960205\n",
      "14.0625 0.449943482875824\n",
      "15.625 0.4695727229118347\n",
      "7.8125 0.45143550634384155\n",
      "14.0625 0.4522177278995514\n",
      "18.75 0.4480416476726532\n",
      "10.9375 0.4606526494026184\n",
      "20.3125 0.43711674213409424\n",
      "18.75 0.4236806631088257\n",
      "6.25 0.481046199798584\n",
      "15.625 0.4527301490306854\n",
      "7.8125 0.4099254906177521\n",
      "12.5 0.4506639242172241\n",
      "15.625 0.45390400290489197\n",
      "7.8125 0.4512729346752167\n",
      "12.5 0.4922569990158081\n",
      "10.9375 0.47079750895500183\n",
      "17.1875 0.41387319564819336\n",
      "23.4375 0.4148295819759369\n",
      "4.6875 0.45185747742652893\n",
      "4.6875 0.477477103471756\n",
      "14.0625 0.42634618282318115\n",
      "17.1875 0.4351571798324585\n",
      "9.375 0.4794468283653259\n",
      "10.9375 0.44324982166290283\n",
      "15.625 0.44664597511291504\n",
      "14.0625 0.45947951078414917\n",
      "15.625 0.4713720679283142\n",
      "18.75 0.4272717833518982\n",
      "15.625 0.4288557171821594\n",
      "14.0625 0.4800431728363037\n",
      "15.625 0.45674991607666016\n",
      "20.3125 0.4182029962539673\n",
      "12.5 0.4482564330101013\n",
      "18.75 0.4289351999759674\n",
      "20.3125 0.4201173186302185\n",
      "23.4375 0.4138166904449463\n",
      "12.5 0.440962553024292\n",
      "18.75 0.40352320671081543\n",
      "15.625 0.4268423914909363\n",
      "21.875 0.40472668409347534\n",
      "14.0625 0.4434615969657898\n",
      "12.5 0.45768100023269653\n",
      "26.5625 0.3968283534049988\n",
      "10.9375 0.4359298348426819\n",
      "10.9375 0.4490000605583191\n",
      "12.5 0.44068846106529236\n",
      "18.75 0.4518683850765228\n",
      "17.1875 0.4296170175075531\n",
      "14.0625 0.4569958448410034\n",
      "21.875 0.415429949760437\n",
      "18.75 0.4359090030193329\n",
      "12.5 0.4786202013492584\n",
      "14.0625 0.4306594729423523\n",
      "20.3125 0.46513646841049194\n",
      "15.625 0.42285364866256714\n",
      "14.0625 0.4319489896297455\n",
      "15.625 0.4796534776687622\n",
      "18.75 0.39595696330070496\n",
      "17.1875 0.4432659149169922\n",
      "21.875 0.43728774785995483\n",
      "18.75 0.42076095938682556\n",
      "23.4375 0.4391070604324341\n",
      "14.0625 0.4545867443084717\n",
      "15.625 0.44110533595085144\n",
      "15.625 0.4556124210357666\n",
      "17.1875 0.42704811692237854\n",
      "20.3125 0.41874831914901733\n",
      "12.5 0.4350329041481018\n",
      "20.3125 0.4276717007160187\n",
      "7.8125 0.43494877219200134\n",
      "15.625 0.42477884888648987\n",
      "7.8125 0.42907020449638367\n",
      "12.5 0.47300708293914795\n",
      "18.75 0.4506942629814148\n",
      "28.125 0.38789960741996765\n",
      "15.625 0.4356236159801483\n",
      "14.0625 0.4560581147670746\n",
      "17.1875 0.39286819100379944\n",
      "17.1875 0.4436672031879425\n",
      "17.1875 0.4474782347679138\n",
      "25.0 0.4243847727775574\n",
      "17.1875 0.44754379987716675\n",
      "18.75 0.4311113953590393\n",
      "15.625 0.4304060637950897\n",
      "23.4375 0.4023973047733307\n",
      "17.1875 0.402870774269104\n",
      "17.1875 0.42676761746406555\n",
      "15.625 0.4272429943084717\n",
      "28.125 0.43855950236320496\n",
      "25.0 0.3888089060783386\n",
      "15.625 0.41588646173477173\n",
      "15.625 0.451869398355484\n",
      "20.3125 0.4390035569667816\n",
      "20.3125 0.40872517228126526\n",
      "37.5 0.3404802978038788\n",
      "15.625 0.392132431268692\n",
      "25.0 0.4004862308502197\n",
      "26.5625 0.40062204003334045\n",
      "15.625 0.41677460074424744\n",
      "15.625 0.4135425388813019\n",
      "10.9375 0.44583722949028015\n",
      "15.625 0.45491406321525574\n",
      "20.3125 0.4180689752101898\n",
      "31.25 0.4000769853591919\n",
      "25.0 0.3799605369567871\n",
      "21.875 0.38616469502449036\n",
      "23.4375 0.4213196635246277\n",
      "15.625 0.42475223541259766\n",
      "21.875 0.3976227045059204\n",
      "18.75 0.40813735127449036\n",
      "21.875 0.408944696187973\n",
      "10.9375 0.4527166187763214\n",
      "18.75 0.41208529472351074\n",
      "20.3125 0.4516723155975342\n",
      "14.0625 0.40961334109306335\n",
      "14.0625 0.42103153467178345\n",
      "21.875 0.39268743991851807\n",
      "25.0 0.4016312062740326\n",
      "25.0 0.4114440381526947\n",
      "20.3125 0.4142313599586487\n",
      "15.625 0.41433870792388916\n",
      "15.625 0.408093124628067\n",
      "25.0 0.375700980424881\n",
      "29.6875 0.41121476888656616\n",
      "17.1875 0.4161008894443512\n",
      "25.0 0.37112459540367126\n",
      "23.4375 0.36133909225463867\n",
      "26.5625 0.38395240902900696\n",
      "17.1875 0.4320776164531708\n",
      "28.125 0.40449416637420654\n",
      "17.1875 0.43045949935913086\n",
      "23.4375 0.4118027091026306\n",
      "21.875 0.3777334988117218\n",
      "23.4375 0.36821672320365906\n",
      "29.6875 0.3754134476184845\n",
      "21.875 0.40875881910324097\n",
      "23.4375 0.37749335169792175\n",
      "21.875 0.4237087070941925\n",
      "21.875 0.41257303953170776\n",
      "26.5625 0.3940458297729492\n",
      "21.875 0.3728162348270416\n",
      "20.3125 0.38002127408981323\n",
      "20.3125 0.3827664852142334\n",
      "21.875 0.39156246185302734\n",
      "25.0 0.3893234133720398\n",
      "31.25 0.3545076847076416\n",
      "17.1875 0.399762362241745\n",
      "18.75 0.42438313364982605\n",
      "15.625 0.41752034425735474\n",
      "21.875 0.410582959651947\n",
      "20.3125 0.3948938548564911\n",
      "31.25 0.3413963317871094\n",
      "23.4375 0.402218222618103\n",
      "18.75 0.4152369499206543\n",
      "6.25 0.509407103061676\n",
      "10.9375 0.47452670335769653\n",
      "12.5 0.5228538513183594\n",
      "3.125 0.5335641503334045\n",
      "7.8125 0.4873805046081543\n",
      "10.9375 0.4833409786224365\n",
      "9.375 0.45491617918014526\n",
      "15.625 0.4641057252883911\n",
      "12.5 0.4616483449935913\n",
      "12.5 0.4270831048488617\n",
      "12.5 0.46690434217453003\n",
      "21.875 0.4175598919391632\n",
      "23.4375 0.4148504137992859\n",
      "14.0625 0.4444178640842438\n",
      "21.875 0.418395072221756\n",
      "10.9375 0.4068145453929901\n",
      "21.875 0.4106329679489136\n",
      "18.75 0.40489497780799866\n",
      "9.375 0.4663526117801666\n",
      "18.75 0.4453067481517792\n",
      "14.0625 0.4132125973701477\n",
      "10.9375 0.432980477809906\n",
      "26.5625 0.38643598556518555\n",
      "20.3125 0.39671462774276733\n",
      "29.6875 0.38512560725212097\n",
      "20.3125 0.3982558250427246\n",
      "29.6875 0.36181071400642395\n",
      "25.0 0.41412824392318726\n",
      "18.75 0.41852906346321106\n",
      "20.3125 0.4218112528324127\n",
      "15.625 0.43178457021713257\n",
      "21.875 0.4089561998844147\n",
      "10.9375 0.42449355125427246\n",
      "28.125 0.4091605246067047\n",
      "26.5625 0.3981362581253052\n",
      "14.0625 0.45663803815841675\n",
      "20.3125 0.4048057794570923\n",
      "21.875 0.3754420280456543\n",
      "23.4375 0.41022613644599915\n",
      "25.0 0.37639671564102173\n",
      "29.6875 0.3786159157752991\n",
      "17.1875 0.4392305612564087\n",
      "20.3125 0.40281692147254944\n",
      "20.3125 0.3877852261066437\n",
      "23.4375 0.35103926062583923\n",
      "18.75 0.41354134678840637\n",
      "28.125 0.3674631714820862\n",
      "15.625 0.4192536473274231\n",
      "26.5625 0.3705209493637085\n",
      "32.8125 0.37629497051239014\n",
      "15.625 0.3986027240753174\n",
      "23.4375 0.4064052999019623\n",
      "23.4375 0.4081284701824188\n",
      "23.4375 0.3872849643230438\n",
      "26.5625 0.359660804271698\n",
      "14.0625 0.45128965377807617\n",
      "17.1875 0.4265028238296509\n",
      "21.875 0.44227439165115356\n",
      "25.0 0.39709722995758057\n",
      "28.125 0.37650951743125916\n",
      "26.5625 0.4240448772907257\n",
      "32.8125 0.3753327429294586\n",
      "32.8125 0.3505147099494934\n",
      "26.5625 0.3688727915287018\n",
      "23.4375 0.3774515390396118\n",
      "25.0 0.3946579694747925\n",
      "15.625 0.4223443865776062\n",
      "25.0 0.38777416944503784\n",
      "18.75 0.38825735449790955\n",
      "17.1875 0.4059677720069885\n",
      "28.125 0.3801767826080322\n",
      "18.75 0.378602534532547\n",
      "17.1875 0.39186614751815796\n",
      "20.3125 0.3696559965610504\n",
      "15.625 0.41793403029441833\n",
      "35.9375 0.3425164520740509\n",
      "23.4375 0.40512603521347046\n",
      "21.875 0.3726297914981842\n",
      "20.3125 0.40068918466567993\n",
      "28.125 0.3985055685043335\n",
      "29.6875 0.36338138580322266\n",
      "21.875 0.39555343985557556\n",
      "31.25 0.3460773825645447\n",
      "35.9375 0.3605726957321167\n",
      "21.875 0.4001341462135315\n",
      "23.4375 0.3670504093170166\n",
      "18.75 0.4063614308834076\n",
      "15.625 0.399273157119751\n",
      "18.75 0.37633803486824036\n",
      "31.25 0.3773137331008911\n",
      "15.625 0.42591315507888794\n",
      "25.0 0.3554810583591461\n",
      "17.1875 0.4143185615539551\n",
      "25.0 0.4184934198856354\n",
      "18.75 0.43799954652786255\n",
      "32.8125 0.34074729681015015\n",
      "31.25 0.3792893886566162\n",
      "31.25 0.34233054518699646\n",
      "29.6875 0.3481708765029907\n",
      "21.875 0.38805678486824036\n",
      "25.0 0.4061039388179779\n",
      "28.125 0.3728756904602051\n",
      "17.1875 0.35866180062294006\n",
      "32.8125 0.3611217439174652\n",
      "20.3125 0.4123525023460388\n",
      "17.1875 0.39721250534057617\n",
      "31.25 0.3238111138343811\n",
      "23.4375 0.3708559572696686\n",
      "25.0 0.3746562600135803\n",
      "25.0 0.34908223152160645\n",
      "21.875 0.37596723437309265\n",
      "29.6875 0.34660765528678894\n",
      "18.75 0.3884679079055786\n",
      "20.3125 0.3936651349067688\n",
      "28.125 0.35147151350975037\n",
      "20.3125 0.3751124441623688\n",
      "25.0 0.3679584860801697\n",
      "28.125 0.3566465377807617\n",
      "26.5625 0.3763102889060974\n",
      "28.125 0.37085646390914917\n",
      "31.25 0.327077180147171\n",
      "26.5625 0.36790743470191956\n",
      "21.875 0.37805649638175964\n",
      "32.8125 0.3525840640068054\n",
      "20.3125 0.3911789357662201\n",
      "32.8125 0.3342878818511963\n",
      "21.875 0.3851110637187958\n",
      "25.0 0.3827950060367584\n",
      "25.0 0.39020368456840515\n",
      "39.0625 0.35166922211647034\n",
      "25.0 0.37415263056755066\n",
      "31.25 0.3546561598777771\n",
      "18.75 0.3875005543231964\n",
      "21.875 0.4099956750869751\n",
      "32.8125 0.33412298560142517\n",
      "26.5625 0.38237279653549194\n",
      "26.5625 0.39321452379226685\n",
      "21.875 0.4078919589519501\n",
      "32.8125 0.3323309123516083\n",
      "31.25 0.3926240801811218\n",
      "31.25 0.36363571882247925\n",
      "26.5625 0.37001582980155945\n",
      "28.125 0.36656537652015686\n",
      "31.25 0.3396618664264679\n",
      "31.25 0.3476389944553375\n",
      "42.1875 0.33245089650154114\n",
      "31.25 0.3451637327671051\n",
      "32.8125 0.33912190794944763\n",
      "28.125 0.35564467310905457\n",
      "26.5625 0.38236483931541443\n",
      "21.875 0.37627112865448\n",
      "34.375 0.37758174538612366\n",
      "25.0 0.37126341462135315\n",
      "25.0 0.38983169198036194\n",
      "26.5625 0.38336068391799927\n",
      "15.625 0.39509332180023193\n",
      "20.3125 0.4079529047012329\n",
      "28.125 0.40050485730171204\n",
      "29.6875 0.3624740540981293\n",
      "23.4375 0.36137086153030396\n",
      "26.5625 0.36595243215560913\n",
      "25.0 0.3692348599433899\n",
      "31.25 0.34368160367012024\n",
      "26.5625 0.37772008776664734\n",
      "26.5625 0.366359144449234\n",
      "23.4375 0.39305543899536133\n",
      "29.6875 0.3696509003639221\n",
      "17.1875 0.39203664660453796\n",
      "26.5625 0.3725525140762329\n",
      "31.25 0.35422542691230774\n",
      "25.0 0.3475949466228485\n",
      "26.5625 0.4020943343639374\n",
      "42.1875 0.3041110336780548\n",
      "28.125 0.353524386882782\n",
      "25.0 0.3761511743068695\n",
      "32.8125 0.3679739832878113\n",
      "25.0 0.3807028830051422\n",
      "31.25 0.3684515655040741\n",
      "26.5625 0.3653997480869293\n",
      "23.4375 0.36386898159980774\n",
      "31.25 0.3295455574989319\n",
      "21.875 0.3783038258552551\n",
      "40.625 0.322848379611969\n",
      "23.4375 0.4064304530620575\n",
      "20.3125 0.41190072894096375\n",
      "28.125 0.3628405034542084\n",
      "31.25 0.3587254583835602\n",
      "35.9375 0.3426111340522766\n",
      "31.25 0.31836503744125366\n",
      "25.0 0.4224732518196106\n",
      "28.125 0.39517742395401\n",
      "29.6875 0.3351042568683624\n",
      "42.1875 0.2830256521701813\n",
      "29.6875 0.3647061288356781\n",
      "39.0625 0.3251909911632538\n",
      "28.125 0.3684026300907135\n",
      "26.5625 0.39658123254776\n",
      "34.375 0.3410913646221161\n",
      "25.0 0.34705936908721924\n",
      "34.375 0.35982728004455566\n",
      "20.3125 0.37476158142089844\n",
      "31.25 0.3477235436439514\n",
      "32.8125 0.328004390001297\n",
      "25.0 0.3454166054725647\n",
      "28.125 0.36385464668273926\n",
      "26.5625 0.3529254198074341\n",
      "29.6875 0.38757795095443726\n",
      "21.875 0.36629340052604675\n",
      "32.8125 0.33972641825675964\n",
      "35.9375 0.3647449314594269\n",
      "31.25 0.3532394468784332\n",
      "25.0 0.3550006151199341\n",
      "25.0 0.33186033368110657\n",
      "29.6875 0.33839619159698486\n",
      "21.875 0.3970429003238678\n",
      "32.8125 0.3610469102859497\n",
      "34.375 0.3300083577632904\n",
      "29.6875 0.3596677780151367\n",
      "20.3125 0.40466341376304626\n",
      "31.25 0.34574267268180847\n",
      "14.0625 0.40915679931640625\n",
      "29.6875 0.38349488377571106\n",
      "39.0625 0.32774046063423157\n",
      "23.4375 0.4052131474018097\n",
      "37.5 0.32601484656333923\n",
      "32.8125 0.3254767954349518\n",
      "29.6875 0.32918581366539\n",
      "29.6875 0.33512410521507263\n",
      "35.9375 0.3162895739078522\n",
      "26.5625 0.3633746802806854\n",
      "40.625 0.3307145833969116\n",
      "26.5625 0.3280167579650879\n",
      "32.8125 0.3182087242603302\n",
      "29.6875 0.3233496844768524\n",
      "25.0 0.4074791967868805\n",
      "28.125 0.3431580364704132\n",
      "35.9375 0.3210292458534241\n",
      "29.6875 0.3572557866573334\n",
      "26.5625 0.3492516875267029\n",
      "28.125 0.3970004916191101\n",
      "37.5 0.3020731806755066\n",
      "39.0625 0.312524676322937\n",
      "25.0 0.3635769188404083\n",
      "32.8125 0.34602090716362\n",
      "43.75 0.30255287885665894\n",
      "31.25 0.34658682346343994\n",
      "18.75 0.3911207914352417\n",
      "34.375 0.3190280497074127\n",
      "28.125 0.37628480792045593\n",
      "35.9375 0.31633585691452026\n",
      "23.4375 0.35682082176208496\n",
      "28.125 0.33915096521377563\n",
      "35.9375 0.33994239568710327\n",
      "26.5625 0.3902437090873718\n",
      "21.875 0.3603020906448364\n",
      "20.3125 0.4089359641075134\n",
      "31.25 0.3575870990753174\n",
      "32.8125 0.367525190114975\n",
      "32.8125 0.3299972414970398\n",
      "32.8125 0.34398144483566284\n",
      "34.375 0.3275071382522583\n",
      "26.5625 0.33910688757896423\n",
      "32.8125 0.31514614820480347\n",
      "31.25 0.3772423267364502\n",
      "37.5 0.32165709137916565\n",
      "35.9375 0.3306584358215332\n",
      "28.125 0.35046425461769104\n",
      "31.25 0.31979888677597046\n",
      "31.25 0.3440190553665161\n",
      "34.375 0.345896452665329\n",
      "29.6875 0.3238975703716278\n",
      "35.9375 0.32674676179885864\n",
      "39.0625 0.3165603578090668\n",
      "28.125 0.3749661445617676\n",
      "34.375 0.34786608815193176\n",
      "23.4375 0.35209327936172485\n",
      "39.0625 0.32082128524780273\n",
      "29.6875 0.3579564690589905\n",
      "29.6875 0.3692777454853058\n",
      "39.0625 0.3142353892326355\n",
      "34.375 0.3059530556201935\n",
      "29.6875 0.3489671051502228\n",
      "32.8125 0.3134877681732178\n",
      "39.0625 0.3064623475074768\n",
      "40.625 0.3171229660511017\n",
      "35.9375 0.33195269107818604\n",
      "37.5 0.31903326511383057\n",
      "29.6875 0.35288798809051514\n",
      "25.0 0.34605568647384644\n",
      "35.9375 0.2815573811531067\n",
      "31.25 0.3583911955356598\n",
      "35.9375 0.335642546415329\n",
      "34.375 0.3473729193210602\n",
      "40.625 0.29806962609291077\n",
      "32.8125 0.3753429651260376\n",
      "26.5625 0.3254498839378357\n",
      "34.375 0.33389028906822205\n",
      "42.1875 0.32767000794410706\n",
      "31.25 0.35573822259902954\n",
      "37.5 0.32592904567718506\n",
      "31.25 0.3697168827056885\n",
      "28.125 0.3328728675842285\n",
      "37.5 0.33637821674346924\n",
      "29.6875 0.3467157483100891\n",
      "35.9375 0.33185654878616333\n",
      "28.125 0.34709903597831726\n",
      "29.6875 0.33897942304611206\n",
      "45.3125 0.2860143482685089\n",
      "37.5 0.35064607858657837\n",
      "29.6875 0.30302220582962036\n",
      "39.0625 0.2934993505477905\n",
      "42.1875 0.27093279361724854\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = ResNet18()\n",
    "model.to(device)\n",
    "num_runs = 2\n",
    "\n",
    "train_loader, test_loader = get_cifar100_loaders(64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for run in range(num_runs):\n",
    "\ttrain(model, 'cuda', train_loader, test_loader, criterion, optimizer, run, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
